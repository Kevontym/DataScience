# Data Processing & ML Pipeline

A scalable data processing and machine learning pipeline for customer data analysis, built with PyTorch and containerized with Docker.

## ğŸš€ Features

- **Data Processing**: Extract, transform, and clean customer data
- **Machine Learning**: Anomaly detection and feature engineering
- **Smart Data Repair**: Automated data quality improvement
- **Containerized**: Fully Dockerized for easy deployment
- **Modular Architecture**: Extensible pipeline components

## ğŸ“ Project Structure
DATA_CLEANER/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ processed/ # Cleaned data files
â”‚ â””â”€â”€ raw/ # Raw input data
â”œâ”€â”€ data_pipeline/ # ETL pipeline components
â”‚ â”œâ”€â”€ extractors/ # Data extraction modules
â”‚ â”œâ”€â”€ transformers/ # Data transformation logic
â”‚ â”œâ”€â”€ loaders/ # Data loading modules
â”‚ â””â”€â”€ utils/ # Utility functions
â”œâ”€â”€ ml_pipeline/ # Machine learning components
â”‚ â”œâ”€â”€ anomaly_detector_ml.py
â”‚ â”œâ”€â”€ feature_engineer.py
â”‚ â””â”€â”€ smart_repair.py
â”œâ”€â”€ Dockerfile # Container configuration
â”œâ”€â”€ docker-compose.yml # Multi-service setup
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ main.py # Main application entry point
â”œâ”€â”€ run.py # Alternative runner script
â””â”€â”€ run-docker.sh # Docker execution script




## ğŸ› ï¸ Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.8+ (for local development)

### Using Docker (Recommended)

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd DATA_CLEANER

    # Or
   ./run-docker.sh


Set up Python environment

bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
Install dependencies

bash
pip install -r requirements.txt
Run the pipeline

bash
python main.py
# or
python run.py



Data Format
The pipeline processes customer data files with the naming convention:

cleaned_customer_data_pytorch_YYYYMM.csv

Processed data is stored in the data/processed/ directory with timestamps.

ğŸ”§ Configuration
Environment Variables
Create a .env file for configuration:

env
DATA_PATH=/app/data
DB_PATH=/app/report_gen.db
LOG_LEVEL=INFO
Docker Configuration
Modify docker-compose.yml for:

Resource limits

Volume mounts

Network settings

ğŸ§ª Testing
Generate sample data for testing:

bash
python create_sample_data.py
ğŸ“ˆ ML Pipeline Components
Anomaly Detection
Identifies unusual patterns in customer data

# Configurable detection thresholds

###Feature Engineering
###Automated feature creation and selection

# Support for temporal features

Smart Repair
Automated data correction

Missing value imputation

Data validation rules

# ğŸ—ƒï¸ Database
The application uses SQLite (report_gen.db) for storing:

Processing metadata

Pipeline execution logs

Analysis reports

# ğŸš¢ Deployment
Production Deployment
Build the image

bash
docker build -t data-cleaner:latest .
Run with production settings

bash
docker-compose -f docker-compose.prod.yml up -d
Kubernetes (Optional)
See the k8s/ directory for Kubernetes manifests.

# ğŸ¤ Contributing
Fork the repository

Create a feature branch (git checkout -b feature/amazing-feature)

Commit your changes (git commit -m 'Add amazing feature')

Push to the branch (git push origin feature/amazing-feature)

Open a Pull Request

# ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.